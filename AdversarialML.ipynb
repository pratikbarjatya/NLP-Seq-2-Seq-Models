{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"AdversarialML.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gaxWjUomeBkp","colab_type":"text"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"360\" height=\"160\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"8Grl3aUqVG6J","colab_type":"text"},"source":["# Adversarial ML : How to attack and defend ML models."]},{"cell_type":"markdown","metadata":{"id":"4-Svy8y_VG6M","colab_type":"text"},"source":["Nowadays, machine learning models in computer vision are used in many real-world applications, like self-driving cars, face recognition, cancer diagnosis, or even in next-generaion shops in order to track which products customers take off the shelf so their credit card can be charged when leaving."]},{"cell_type":"markdown","metadata":{"id":"2BjfIHtbVG6N","colab_type":"text"},"source":["The increasing accuracy of these machine learning systems is quite impressive, so it naturally led to a varitable flood of applications using them.\n","\n","Although the mathematical foundations behond them were already studied a few decades ago, the relatively recent advent of powerful GPU gave researchers the computing power necessary to experiment and build complex machine learning systems.\n","\n","Today state-of-the art models for computer vision are based on deep networks with up to several million parameters, and they rely on hardware that was not available just a decade ago."]},{"cell_type":"markdown","metadata":{"id":"EXJkIXKPVG6O","colab_type":"text"},"source":["### So how about tricking Neural networks?"]},{"cell_type":"markdown","metadata":{"id":"_bFkmQ0QVG6Q","colab_type":"text"},"source":["Assassination by neural network. It must be sounding crazy?\n","\n","- Well, it might happen someday, and not in the way we think.\n","\n","- Of course, neural networks could be trained to pilot drones or operate other weapons of mass destruction, but even an innocuos (and presently available) network trained to drive a car could be turned to act against its owner.\n","\n","- This is because neural networks are extremely susceptible to something called adversarial examples."]},{"cell_type":"markdown","metadata":{"id":"d7USbO9bVG6R","colab_type":"text"},"source":["Adversarial examples are inputs to a neural network that result in an incorrect output from the network.\n","\n","It’s probably best to show an example.\n","\n","- We can start with an image of a panda on the left which some network thinks with 57.7% confidence is a “panda.”\n","\n","- The panda category is also the category with the highest confidence out of all the categories, so the network concludes that the object in the image is a panda.\n","\n","- But then by adding a very small amount of carefully constructed noise you can get an image that looks exactly the same to a human, but that the network thinks with 99.3% confidence is a “gibbon.” \n","\n","Pretty crazy stuff!"]},{"cell_type":"markdown","metadata":{"id":"tBl0X3IvVG6T","colab_type":"text"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/am1.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"QlJtkKQbVG6V","colab_type":"text"},"source":["So just how would assassination by adversarial example work?\n","\n","- Imagine replacing a stop sign with an adversarial example of it–that is, a sign that a human would recognize instantly but a neural network would not even register.\n","\n","- Now imagine placing that adversarial stop sign at a busy intersection.\n","\n","- As self-driving cars approach the intersection the on-board neural networks would fail to see the stop sign and continue right into oncoming traffic, bringing its occupants to near certain death (in theory)."]},{"cell_type":"markdown","metadata":{"id":"FuBvGG4iVG6Y","colab_type":"text"},"source":["Now, this might just be one convoluted and (more than) slightly sensationalized instance of how people could use adversarial examples for harm, but there are many more.\n","\n","For example, the iPhone X’s “Face ID” unlocking feature relies on neural nets to recognize faces and is therefore susceptible to adversarial attacks.\n","\n","- People could construct adversarial images to bypass the Face ID security features.\n","\n","<br> \n","Other biometric security systems would also be at risk and illegal or improper content could potentially bypass neural-network-based content filters by using adversarial examples.\n","\n","The existence of these adversarial examples means that systems that incorporate deep learning models actually have a very high-security risk."]},{"cell_type":"markdown","metadata":{"id":"fbmdOC4WVG6Z","colab_type":"text"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/am2.jpeg\"></center>"]},{"cell_type":"markdown","metadata":{"id":"gS87Aq-MVG6c","colab_type":"text"},"source":["The above adversarial example with the panda is a **targeted** example.\n","\n","- A small amount of carefully constructed noise was added to an image that caused a neural network to misclassify the image, despite the image looking exactly the same to a human.\n","\n","<br> \n","There are also **non-targeted** examples which simply try to find any input that tricks the neural network.\n","\n","- This input will probably look like white noise to a human, but because we aren’t constrained to find an input that resembles something to a human the problem is a lot easier."]},{"cell_type":"markdown","metadata":{"id":"SooJ2N5VVG6e","colab_type":"text"},"source":["We can find adversarial examples for just about any neural network out there, even state-of-the-art models that have so-called “superhuman” abilities, which is slightly troubling."]},{"cell_type":"markdown","metadata":{"id":"nyp7_KN6VG6f","colab_type":"text"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/am3.jpeg\"></center>"]},{"cell_type":"markdown","metadata":{"id":"HQuRZa9oVG6h","colab_type":"text"},"source":["**How can staet-of-the art models having human level classification accuracy, make such seemingly stupid mistakes?**\n","\n","<br> \n","Before we delve into the weakness that neural network models tend to have, let us remeber that we humans have our own set of adversarial examples.\n","\n","Take a look at the image below. What do you see?\n","\n","- A spiral or a series of concentric circles?"]},{"cell_type":"markdown","metadata":{"id":"2pjzrKXaVG6i","colab_type":"text"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/am.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"0C5bsG3SVG6j","colab_type":"text"},"source":["What this example show is that machine learning models and human vision must be using quite diferent internal representations when understanding what it is there in an image."]},{"cell_type":"markdown","metadata":{"id":"FEjfIyX-VG6k","colab_type":"text"},"source":["### How to Generate Adversarial Examples?"]},{"cell_type":"markdown","metadata":{"id":"tQj2s1JsVG6m","colab_type":"text"},"source":["Adversarial examples are generated by taking a clean image that the model correctly classifies, and finding a small perturbation that causes the new image to be misclassified by the ML model."]},{"cell_type":"markdown","metadata":{"id":"6fEILGYnVG6n","colab_type":"text"},"source":["The higher the dimension of the input image space is, the easier it is to generate adversarial examples that are indistinguishable from clean images by the human eye.\n","\n","The different method for doing so are:\n","\n","1. L-BFGS METHOD\n","\n","2. FAST GRADIENT SIGN (FGS)\n","\n","3. ITERATIVE FAST GRADIENT SIGN\n","\n","4. BLACK BOX ATTACK"]},{"cell_type":"markdown","metadata":{"id":"uHqMEEA3VG6o","colab_type":"text"},"source":["### Defenses against Adversarial Examples"]},{"cell_type":"markdown","metadata":{"id":"XvY15aOyVG6p","colab_type":"text"},"source":["The attacker crafts the attack, exploiting all the information they have about the model.\n","\n","Obviously, the less information the model outputs at prediction time, the harder it is for an attacker to craft a successful attack."]},{"cell_type":"markdown","metadata":{"id":"7_K1WyRtVG6q","colab_type":"text"},"source":["- Easy measure to protect classification model in a production environment to avoid showing confidence scores for each predicted class.\n","\n","- Instead, model should only provide the top N most likely classes.\n","\n","- When confidence scores are provided to the end user, a malicious attacker can use them to numerically estimate the gradient of the loss function."]},{"cell_type":"markdown","metadata":{"id":"AQSUbLCvVG6r","colab_type":"text"},"source":["### Defensive Distillation"]},{"cell_type":"markdown","metadata":{"id":"DBqFEDxUVG6s","colab_type":"text"},"source":["It generates a new model whose gradients are much smaller that the original undefended model.\n","\n","If gradients are very small, techniques like FGS or Iterative FGS are no longer useful, as the attacker would need great distortions of the input image to achieve a sufficient change in the loss function."]},{"cell_type":"markdown","metadata":{"id":"Z-cIq_f0VG6t","colab_type":"text"},"source":["Defensive distillation proceeds as follows:\n","\n","1. Train a network, called the teacher network, with a temperature T>>1.\n","\n","2. Use the trained teacher network to generate soft-labels for each image in the training set. A soft label for an image is the set of probabilities that the model assigns to each class.\n","\n","    - For example, if the output image is a parrot, the teacher model might output soft labels like (90% parrot, 10% papagayo).\n","\n","3. Train a second network, the distilled network, on the soft-labels, using again the temperature T. Training with soft-labels is a technique that reduces overfitting and improves out-of-sample accuracy of the distilled network.\n","\n","4. Finally, at prediction time, run the distilled network with temperature T=1."]},{"cell_type":"markdown","metadata":{"id":"gDCPIye0VG6u","colab_type":"text"},"source":["### Adversarial Training"]},{"cell_type":"markdown","metadata":{"id":"-9hxqmTFVG6v","colab_type":"text"},"source":["Adversarial training is the most effective defense strategy.\n","\n","- It uses a modified loss function that is a weighted sum of the usual loss function on clean examples and a loss function from adversarial examples."]}]}